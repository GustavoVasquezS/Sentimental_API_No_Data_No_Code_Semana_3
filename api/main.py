"""
SentimentAPI v3.0 - API de An√°lisis de Sentimientos en Espa√±ol
==============================================================

API REST para clasificaci√≥n de sentimientos en rese√±as de Amazon en espa√±ol.
Compatible con el modelo `sentiment_bundle.joblib` del notebook Proyecto_final_v8.

Clasificaciones: Positivo, Neutro, Negativo
- Negativo: estrellas 1-2
- Neutro: estrella 3
- Positivo: estrellas 4-5

Caracter√≠sticas:
- Probabilidad calibrada (0-1)
- Flag `review_required` para casos de baja confianza
- Metadata del modelo incluida en cada respuesta

Ejecutar con: uvicorn main:app --reload --port 8000
Documentaci√≥n: http://localhost:8000/docs
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_validator
import joblib
import numpy as np
import os
from typing import Optional, Dict, Any, List
from datetime import datetime
import logging

# Configuraci√≥n de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =============================================================================
# CONFIGURACI√ìN DE LA API
# =============================================================================

app = FastAPI(
    title="SentimentAPI v3.0 - Amazon ES",
    description="""
API de An√°lisis de Sentimientos para rese√±as de Amazon en espa√±ol.

## Caracter√≠sticas
- **Modelo calibrado**: Probabilidades interpretables (0-1)
- **Review autom√°tico**: Flag `review_required` cuando confianza < 60%
- **Trazabilidad**: Versi√≥n del modelo y hash en cada respuesta

## Clasificaci√≥n
| Estrellas | Sentimiento |
|:---------:|:-----------:|
| 4-5 ‚≠ê | Positivo |
| 3 ‚≠ê | Neutro |
| 1-2 ‚≠ê | Negativo |
    """,
    version="3.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configurar CORS para permitir peticiones desde cualquier origen
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =============================================================================
# RUTAS DE ARCHIVOS DEL MODELO
# =============================================================================

# El bundle est√° en la ra√≠z del proyecto (un nivel arriba de api/)
BUNDLE_PATH = os.path.join(os.path.dirname(__file__), "..", "sentiment_bundle.joblib")

# Variables globales para el modelo
bundle: Optional[Dict[str, Any]] = None
pipeline = None
model_meta: Optional[Dict[str, Any]] = None

# Estad√≠sticas de uso
stats = {
    "total_requests": 0,
    "positive_count": 0,
    "negative_count": 0,
    "neutral_count": 0,
    "review_required_count": 0,
    "avg_probability": 0.0,
    "start_time": datetime.now().isoformat()
}

# =============================================================================
# MODELOS PYDANTIC (VALIDACI√ìN)
# =============================================================================

class TextInput(BaseModel):
    """Modelo de entrada para el an√°lisis de sentimiento."""
    text: str = Field(
        ..., 
        min_length=5, 
        max_length=2000, 
        description="Texto a analizar (5-2000 caracteres)"
    )
    
    @field_validator('text')
    @classmethod
    def text_not_empty(cls, v):
        if not v or not v.strip():
            raise ValueError('El texto no puede estar vac√≠o')
        stripped = v.strip()
        if len(stripped) < 5:
            raise ValueError('El texto debe tener al menos 5 caracteres')
        return stripped


class SentimentResponse(BaseModel):
    """Modelo de respuesta del an√°lisis de sentimiento."""
    prevision: str = Field(..., description="Sentimiento predicho: Positivo, Neutro o Negativo")
    probabilidad: float = Field(..., description="Probabilidad/confianza de la predicci√≥n (0-1)")
    review_required: bool = Field(..., description="True si probabilidad < threshold (requiere revisi√≥n humana)")
    threshold: float = Field(..., description="Umbral de confianza configurado")
    model_version: str = Field(..., description="Versi√≥n del modelo")
    artifact_hash: str = Field(..., description="Hash del artefacto para trazabilidad")


class ErrorResponse(BaseModel):
    """Modelo de respuesta de error."""
    error: str = Field(..., description="Tipo de error")
    detail: str = Field(..., description="Descripci√≥n del error")


class HealthResponse(BaseModel):
    """Modelo de respuesta del health check."""
    status: str
    model_loaded: bool
    model_version: Optional[str] = None
    threshold: Optional[float] = None
    labels: Optional[List[str]] = None
    artifact_hash: Optional[str] = None


class StatsResponse(BaseModel):
    """Modelo de respuesta de estad√≠sticas."""
    total_requests: int
    positive_count: int
    negative_count: int
    neutral_count: int
    review_required_count: int
    review_required_percentage: float
    avg_probability: float
    start_time: str


# =============================================================================
# FUNCIONES DE CARGA DEL MODELO
# =============================================================================

def load_bundle():
    """Carga el bundle (pipeline + metadata) al iniciar la API."""
    global bundle, pipeline, model_meta
    
    try:
        if not os.path.exists(BUNDLE_PATH):
            raise FileNotFoundError(f"No se encontr√≥ el archivo: {BUNDLE_PATH}")
        
        bundle = joblib.load(BUNDLE_PATH)
        pipeline = bundle["pipeline"]
        model_meta = bundle["meta"]
        
        logger.info("‚úÖ Bundle cargado exitosamente")
        logger.info(f"   Versi√≥n: {model_meta['model_version']}")
        logger.info(f"   Labels: {model_meta['labels']}")
        logger.info(f"   Threshold: {model_meta['threshold']}")
        logger.info(f"   Hash: {model_meta['artifact_hash']}")
        
    except Exception as e:
        logger.error(f"‚ùå Error cargando bundle: {e}")
        raise


# =============================================================================
# FUNCI√ìN DE PREDICCI√ìN
# =============================================================================

def predict_sentiment(text: str) -> Dict[str, Any]:
    """
    Predice el sentimiento de un texto.
    
    Args:
        text: Texto a analizar (ya validado)
    
    Returns:
        dict con predicci√≥n, probabilidad y metadata
    """
    global stats
    
    # El pipeline ya incluye TextCleaner, no necesitamos preprocesar
    proba = pipeline.predict_proba([text])[0]
    classes = pipeline.classes_
    
    # Obtener la clase con mayor probabilidad
    idx = int(np.argmax(proba))
    pred = str(classes[idx])
    max_prob = float(proba[idx])
    
    # Determinar si requiere revisi√≥n humana
    threshold = float(model_meta["threshold"])
    needs_review = max_prob < threshold
    
    # Actualizar estad√≠sticas
    stats["total_requests"] += 1
    n = stats["total_requests"]
    stats["avg_probability"] = stats["avg_probability"] + (max_prob - stats["avg_probability"]) / n
    
    if needs_review:
        stats["review_required_count"] += 1
    
    if pred == "Positivo":
        stats["positive_count"] += 1
    elif pred == "Negativo":
        stats["negative_count"] += 1
    else:
        stats["neutral_count"] += 1
    
    return {
        "prevision": pred,
        "probabilidad": round(max_prob, 4),
        "review_required": needs_review,
        "threshold": threshold,
        "model_version": str(model_meta["model_version"]),
        "artifact_hash": str(model_meta["artifact_hash"])
    }


# =============================================================================
# EVENTOS DE CICLO DE VIDA
# =============================================================================

@app.on_event("startup")
async def startup_event():
    """Evento ejecutado al iniciar la API."""
    logger.info("üöÄ Iniciando SentimentAPI v3.0...")
    load_bundle()
    logger.info("‚úÖ API lista para recibir peticiones")


# =============================================================================
# ENDPOINTS
# =============================================================================

@app.get("/", tags=["General"])
async def root():
    """Endpoint ra√≠z con informaci√≥n de la API."""
    return {
        "nombre": "SentimentAPI v3.0 - Amazon ES",
        "version": "3.0.0",
        "descripcion": "API de An√°lisis de Sentimientos para rese√±as de Amazon en espa√±ol",
        "modelo": {
            "version": model_meta["model_version"] if model_meta else None,
            "labels": model_meta["labels"] if model_meta else None,
            "threshold": model_meta["threshold"] if model_meta else None,
            "artifact_hash": model_meta["artifact_hash"] if model_meta else None
        },
        "endpoints": {
            "POST /sentiment": "Analizar sentimiento de un texto",
            "GET /health": "Estado de la API y del modelo",
            "GET /stats": "Estad√≠sticas de uso"
        },
        "docs": "/docs"
    }


@app.get("/health", response_model=HealthResponse, tags=["General"])
async def health_check():
    """Verifica el estado de la API y del modelo."""
    return HealthResponse(
        status="healthy" if pipeline is not None else "unhealthy",
        model_loaded=pipeline is not None,
        model_version=model_meta["model_version"] if model_meta else None,
        threshold=model_meta["threshold"] if model_meta else None,
        labels=model_meta["labels"] if model_meta else None,
        artifact_hash=model_meta["artifact_hash"] if model_meta else None
    )


@app.get("/stats", response_model=StatsResponse, tags=["General"])
async def get_stats():
    """Obtiene estad√≠sticas de uso de la API."""
    total = stats["total_requests"]
    review_pct = (stats["review_required_count"] / total * 100) if total > 0 else 0
    
    return StatsResponse(
        total_requests=total,
        positive_count=stats["positive_count"],
        negative_count=stats["negative_count"],
        neutral_count=stats["neutral_count"],
        review_required_count=stats["review_required_count"],
        review_required_percentage=round(review_pct, 2),
        avg_probability=round(stats["avg_probability"], 4),
        start_time=stats["start_time"]
    )


@app.post(
    "/sentiment", 
    response_model=SentimentResponse,
    responses={
        200: {"description": "Predicci√≥n exitosa"},
        400: {"model": ErrorResponse, "description": "Error de validaci√≥n"},
        500: {"model": ErrorResponse, "description": "Error interno"}
    },
    tags=["Predicci√≥n"]
)
async def analyze_sentiment(input_data: TextInput):
    """
    Analiza el sentimiento de un texto en espa√±ol.
    
    ## Request
    ```json
    { "text": "El producto es excelente y lleg√≥ r√°pido" }
    ```
    
    ## Response
    - **prevision**: Sentimiento predicho (Positivo, Neutro, Negativo)
    - **probabilidad**: Confianza del modelo (0.0 - 1.0)
    - **review_required**: `true` si probabilidad < threshold
    - **threshold**: Umbral de confianza (0.6 por defecto)
    - **model_version**: Versi√≥n del modelo para trazabilidad
    - **artifact_hash**: Hash √∫nico del artefacto
    
    ## Interpretaci√≥n de `review_required`
    - `false`: El modelo est√° seguro, se puede usar la predicci√≥n autom√°ticamente
    - `true`: Baja confianza, se recomienda revisi√≥n humana
    """
    # Verificar que el modelo est√° cargado
    if pipeline is None or model_meta is None:
        raise HTTPException(
            status_code=500, 
            detail="Modelo no cargado. Reinicie la API."
        )
    
    try:
        # Realizar predicci√≥n
        result = predict_sentiment(input_data.text)
        
        # Log de la predicci√≥n
        emoji = "üü¢" if result["prevision"] == "Positivo" else ("üî¥" if result["prevision"] == "Negativo" else "üü°")
        review_flag = "‚ö†Ô∏è" if result["review_required"] else "‚úì"
        logger.info(
            f"{emoji} {result['prevision']} | "
            f"Prob: {result['probabilidad']:.2%} {review_flag} | "
            f"'{input_data.text[:50]}...'"
        )
        
        return SentimentResponse(**result)
        
    except Exception as e:
        logger.error(f"Error en predicci√≥n: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Error procesando el texto: {str(e)}"
        )


# =============================================================================
# ENDPOINT BATCH (PARA M√öLTIPLES TEXTOS)
# =============================================================================

class BatchInput(BaseModel):
    """Entrada para an√°lisis en lote."""
    texts: List[str] = Field(..., min_length=1, max_length=100, description="Lista de textos (m√°x 100)")


class BatchResponse(BaseModel):
    """Respuesta del an√°lisis en lote."""
    results: List[SentimentResponse]
    total: int
    successful: int
    failed: int


@app.post("/sentiment/batch", response_model=BatchResponse, tags=["Predicci√≥n"])
async def analyze_sentiment_batch(input_data: BatchInput):
    """
    Analiza el sentimiento de m√∫ltiples textos en una sola petici√≥n.
    
    M√°ximo 100 textos por petici√≥n.
    """
    if pipeline is None or model_meta is None:
        raise HTTPException(
            status_code=500, 
            detail="Modelo no cargado. Reinicie la API."
        )
    
    results = []
    failed = 0
    
    for text in input_data.texts:
        try:
            # Validar longitud m√≠nima
            if not text or len(text.strip()) < 5:
                failed += 1
                continue
            
            result = predict_sentiment(text.strip())
            results.append(SentimentResponse(**result))
            
        except Exception:
            failed += 1
    
    return BatchResponse(
        results=results,
        total=len(input_data.texts),
        successful=len(results),
        failed=failed
    )


# =============================================================================
# EJECUCI√ìN DIRECTA
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
